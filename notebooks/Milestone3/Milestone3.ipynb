{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this milestone, we will be setting up our spark cluster and developing our machine learning model. These were the tasks that we did:\n",
    " \n",
    "(1) Setup EMR cluster  \n",
    "(2) Develop ML model using scikit-learn  \n",
    "(3) Obtain the best hyperparameters using spark's MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup EMR Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"img/...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup browser, Jupyter environment and connect to Master node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"img/...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Develop ML model using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtain best hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, UnivariateFeatureSelector\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor as sparkRFR\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Depending on the permissions that you provided to your bucket you might need to provide your aws credentials\n",
    "## to read from the bucket, if so provide with your credentials and pass as storage_options=aws_credentials\n",
    "aws_credentials = {\"key\": \"ASIAWBUNGPD6YF426OOH\",\n",
    "                   \"secret\": \"VELZE0QG7HEL1di0Ew79RXXgicYO7v2Q+L4H0VAU\",\n",
    "                   \"token\":\"FwoGZXIvYXdzEKT//////////wEaDJLexc+Emjx42o2UoyLPARCC8xWnUwqpj/P9vrHfKyuuOptrJe04o1GVsgEMZaN3vakDk1nYXFSm1WBVSg+sDt49hPd1e5uXXSUeNa33MfzHmcEtzxPtvS18F0xONjYmzImb+22qhFaPI39TBaWdhqvC2BrwCkSboWaEVtuB4RV4ZrJiJ2Jaw/79q2QHzI2w5wzHOQERfvtif2uMDRgOTZVCJriETgrl1P71nNh2TmgrbO2HktNaaQYEnTJlgqL0pRRYEFU3gnc0uORWrSwlVSKjETWGwSaj92NC94K9oCjP1NOSBjIt2+Kbo8oWa6IARILW7FCilKIx+YNNNwvSuRUjl583U6nAAUAoaM/AKo0gzlgL\"}\n",
    "## here 100 data points for testing the code\n",
    "pandas_df = pd.read_csv(\"s3://mdsfinal/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).iloc[:100].dropna()\n",
    "# pandas_df = pd.read_csv(\"s3://mdsfinal/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).dropna()\n",
    "train_df, test_df = train_test_split(pandas_df, test_size=0.2, random_state=123)\n",
    "feature_cols = list(pandas_df.drop(columns=\"observed_rainfall\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe and coerce features into a single column called \"Features\"\n",
    "# This is a requirement of MLlib\n",
    "# Here we are converting your pandas dataframe to a spark dataframe, \n",
    "# Here \"spark\" is a spark session I will discuss this in our Wed class. \n",
    "# It is automatically created for you in this notebook.\n",
    "# read more  here https://blog.knoldus.com/spark-createdataframe-vs-todf/\n",
    "\n",
    "training = spark.createDataFrame(train_df)\n",
    "testing = spark.createDataFrame(test_df)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = assembler.transform(training).select(\"Features\", \"observed_rainfall\")\n",
    "testing = assembler.transform(training).select(\"Features\", \"observed_rainfall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Once you finish testing the model on 100 data points, then load entire dataset and run , this could take ~15 min.\n",
    "## write code here.\n",
    "rf = RandomForestRegressor(labelCol=\"observed_rainfall\", featuresCol=\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(rf.numTrees, [10, 50, 100]\n",
    "               .addGrid(rf.maxDepth, [5, 10])\n",
    "               .addGrid(rf.bootstrap, [False, True])\n",
    "               )\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfevaluator = RegressionEvaluator(labelCol = 'observed_rainfall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-fold CrossValidator\n",
    "rfcv = CrossValidator(estimator = rf,\n",
    "                      estimatorParamMaps = rfparamGrid,\n",
    "                      evaluator = rfevaluator,\n",
    "                      numFolds = 5)\n",
    "rfcv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = evaluator.evaluate(rfcv.transform(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print run info\n",
    "print(\"\\nBest model\")\n",
    "print(\"==========\")\n",
    "print(f\"\\nCV Score: {min(cvModel.avgMetrics):.2f}\")\n",
    "print(f\"numTrees: {cvModel.bestModel.getNumTrees}\")\n",
    "print(f\"maxDepth: {cvModel.bestModel.getMaxDepth()}\")\n",
    "print(f\"bootstrap: {cvModel.bestModel.getBoostrap()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deployment of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=___, max_depth=___)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train RMSE: {mean_squared_error(y_train, model.predict(X_train), squared=False):.2f}\")\n",
    "print(f\" Test RMSE: {mean_squared_error(y_test, model.predict(X_test), squared=False):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ready to deploy\n",
    "dump(model, \"model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](\"img/...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:new563]",
   "language": "python",
   "name": "conda-env-new563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
