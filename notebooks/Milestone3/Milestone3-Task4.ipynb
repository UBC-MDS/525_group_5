{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Guided Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of Milestone 3, task 4 and is a guided exercise. I have put guidelines and helpful links (as comments) along with this notebook to take you through this.\n",
    "\n",
    "In this exercise you will be using Spark's MLlib. The idea is to tune some hyperparameters of a Random Forest to find an optimum model. Once we know the optimum settings, we'll train a Random Forest in sklearn (task 4)and save it with joblib (task 5) (so that we can use it next week to deploy).\n",
    "\n",
    "Here consider MLlib as another python package that you are using, like the scikit-learn. You will be seeing many scikit-learn similar classes and methods available in MLlib for various ML related tasks, you might also notice that some of them are not yet implimented in MLlib. What you write using pyspark package will be using the spark engine to run your code, and hence all the benefits of distributed computing what we discussed in class.\n",
    "\n",
    "NOTE: Here whenever you use spark makes sure that you refer to the right documentation based on the version what you will be using. [Here](https://spark.apache.org/docs/) you can select the version of the spark and go to the correct documentation. In our case we are using spark 3.1.2, and here is the link to spark documetation that you can refer to,\n",
    "- [MLlib Documentation](https://spark.apache.org/docs/3.1.2/ml-guide.html)\n",
    "- [MLlib API Reference](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.ml.html)\n",
    "\n",
    "You may notice that there are RDD-based API and DataFrame-based (Main Guide) API available in the documentation. You want to focus on DataFrame based API as no one these days use RDD based API. We will discuss the difference in class.\n",
    "\n",
    "Before you start this notebook make sure that you are using EMR jupyterHub and the kernal that you selected is PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, UnivariateFeatureSelector\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor as sparkRFR\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with; read 100 data points for development purpose. Once your code is ready then try on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "## Depending on the permissions that you provided to your bucket you might need to provide your aws credentials\n",
    "## to read from the bucket, if so provide with your credentials and pass as storage_options=aws_credentials\n",
    "# aws_credentials = {\"key\": \"\",\"secret\": \"\",\"token\":\"\"}\n",
    "## here 100 data points for testing the code\n",
    "pandas_df = pd.read_csv(\"s3://mds-s3-5/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).iloc[:100].dropna()\n",
    "# pandas_df = pd.read_csv(\"s3://mds-s3-5/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).dropna()\n",
    "feature_cols = list(pandas_df.drop(columns=\"Observed\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataframe and coerce features into a single column called \"Features\"\n",
    "# This is a requirement of MLlib\n",
    "# Here we are converting your pandas dataframe to a spark dataframe, \n",
    "# Here \"spark\" is a spark session I will discuss this in our Wed class. \n",
    "# It is automatically created for you in this notebook.\n",
    "# read more  here https://blog.knoldus.com/spark-createdataframe-vs-todf/\n",
    "training = spark.createDataFrame(pandas_df)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"Features\")\n",
    "training = assembler.transform(training).select(\"Features\", \"Observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official Documentation of MLlib, Random forest regression [here](http://spark.apache.org/docs/3.0.1/ml-classification-regression.html#random-forest-regression).\n",
    "\n",
    "Here we will be mainly using following classes and methods;\n",
    "\n",
    "- [RandomForestRegressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html)\n",
    "- [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html)\n",
    "    - addGrid\n",
    "    - build\n",
    "- [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html)\n",
    "    - fit\n",
    "\n",
    "Use these parameters for coming up with ideal parameters, you could try more parameters, but make sure you have enough power to do it. But you are required to try only following parameters. This will take around 15 min on entire dataset....\n",
    "\n",
    "    - Use numTrees as [10, 50,100]\n",
    "    - maxDepth as [5, 10]\n",
    "    - bootstrap as [False, True]\n",
    "    - In the CrossValidator use evaluator to be RegressionEvaluator(labelCol=\"Observed\")\n",
    "    \n",
    "***Additional reference:*** You can refer to [here](https://www.sparkitecture.io/machine-learning/regression/random-forest) and [here](https://www.silect.is/blog/random-forest-models-in-spark-ml/).\n",
    "Some additional reading [here](https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "##Once you finish testing the model on 100 data points, then load entire dataset and run , this could take ~15 min.\n",
    "## write code here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Print run info\n",
    "print(\"\\nBest model\")\n",
    "print(\"==========\")\n",
    "print(f\"\\nCV Score: {min(cvModel.avgMetrics):.2f}\")\n",
    "print(f\"numTrees: {cvModel.bestModel.getNumTrees}\")\n",
    "print(f\"numTrees: {cvModel.bestModel.getMaxDepth()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
