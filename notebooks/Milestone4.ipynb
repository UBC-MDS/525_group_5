{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb8edc9-ac2a-418d-82a1-0b2b834a50c3",
   "metadata": {},
   "source": [
    "## 3. Summarize your journey from Milestone 1 to Milestone 4\n",
    "rubric={mechanics:10}\n",
    ">There is no format or structure on how you write this. (also, no minimum number of words).  It's your choice on how well you describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c8d8d-c1ca-49ee-a2a0-e08d0787e091",
   "metadata": {},
   "source": [
    "**Milestone 1**: We worked with a large dataset and recorded the time taken and memory used at each step on our personal computers. To reduce the time and memory usage, we used other optimization techniques such as \"converting numerical columns from float64 to float32\" and \"loading individual columns\" while doing the EDA. Our conclusion was that the M1 chip performed 1.5 times faster compared to the IntelCorei5 chip. Finally, to reduce the time and memory required, we explored different ways of saving a big data set by using the parquet and feather format.\n",
    "\n",
    "**Milestone 2**: Handling large files becomes a hassle when working on our local system. As such, we were introduced to Amazon Web Services (AWS). Here we set up an EC2 instance along with JupyterHub, loaded the required packages and added users to it. We then set up a S3 bucket to store our files. We used the bucket to read and write our files. Finally, we carried out EDA and wrangling of our dataset which we used in Milestone 1. This took significantly less time. Our final product in this milestone was stored in the S3 bucket.\n",
    "\n",
    "**Milestone 3**: In this milestone, we performed machine learning tasks. Since there is a limit to how much we can scale vertically, we scale horizontally by setting up an EMR cluster along with Spark, Hadoop, Jupyterhub along with the packages required. We performed EDA and used random forest regression on both the overall and individual models. Finally, we tuned the overall model, and adjusted several hyperparameters, including the max depth and the number of estimators. This model was stored in our S3 bucket.\n",
    "\n",
    "**Milestone 4**: In this milestone, we deployed the model that we saved in the previous milestone and build the API using Flask. We stored the code in the bucket so that we could use it to perform both the reading and writing tasks and store it in the bucket. After which, we created screen sessions to keep the API from stopping even if the user is offline. The API can now run without Flask."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
